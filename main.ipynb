{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt\n",
    "import math as m\n",
    "import numpy as np\n",
    "import abc\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512 # embeddings dim\n",
    "heads = 8 # number of heads for multihead attention\n",
    "ed_count = 6 # N, number of encoder-decoder layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 185749.02it/s]\n"
     ]
    }
   ],
   "source": [
    "def TTV_split(data, tr, te, va, verbose=False):\n",
    "    total = len(data)\n",
    "    one = total / (tr + te + va)\n",
    "\n",
    "    tr_n = m.floor(tr * one)\n",
    "    te_n = m.floor(te * one)\n",
    "    va_n = m.floor(va * one)\n",
    "\n",
    "    tr_n += (total - tr_n - te_n - va_n)\n",
    "\n",
    "    train_data = np.array([])\n",
    "    test_data = np.array([])\n",
    "    val_data = np.array([])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    for i in tqdm(data):\n",
    "        if (len(train_data) < tr_n):\n",
    "            np.append(train_data, i)\n",
    "            continue\n",
    "        if (len(test_data) < te_n):\n",
    "            np.append(test_data, i)\n",
    "            continue\n",
    "        if (len(val_data) < va_n):\n",
    "            np.append(val_data, i)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"total = {total}\\ntr_n = {len(train_data)} \\nte_n = {len(test_data)} \\nva_n = {len(val_data)}\")\n",
    "    return \n",
    "\n",
    "TTV_split(np.array([i for i in range(100000)]), 12, 4, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Module Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnModule(): # TODO: добавить тесты для каждого модуля\n",
    "    def __init__(self, params={}) -> None:\n",
    "        self.params = params\n",
    "\n",
    "    @abc.abstractclassmethod\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractclassmethod\n",
    "    def backward(self, grad):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractclassmethod\n",
    "    def step(self, lr):\n",
    "        pass\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        rs = \"\"\n",
    "        strs = np.array([f\"{i}: {self.params[i]}, \" for i in self.params])\n",
    "        for i in strs:\n",
    "            rs += i\n",
    "        return (f\"{self.name}: (\" + rs[:len(rs)-2] + \")\") if self.params else self.name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "        try:\n",
    "            if params[\"in_dim\"] < 1 or params[\"out_dim\"] < 1:\n",
    "                raise Exception\n",
    "        except KeyError:\n",
    "            raise Exception(\"You have to set in_dim and out_dim parameters for linear layer\")\n",
    "        except Exception:\n",
    "            raise Exception(\"The in_dim and out_dim have to be greater than zero\")\n",
    "\n",
    "        self.name = \"Linear\"\n",
    "\n",
    "        self._res = None\n",
    "        self._lastX = None\n",
    "\n",
    "        self.W = np.ones(params[\"in_dim\"], params[\"out_dim\"], dtype=np.float32)\n",
    "        self.B = np.zeros(params[\"out_dim\"], dtype=np.float32)\n",
    "\n",
    "        self._grad_weight = None\n",
    "        self._grad_bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._lastX = x\n",
    "        self._res = np.dot(x, self.W) + self.B\n",
    "        return self._res\n",
    "\n",
    "    def backward(self, grad):\n",
    "        self._grad_weight = np.dot(self._lastX.T, grad)\n",
    "        self._grad_bias = np.sum(grad, axis=0)\n",
    "\n",
    "    def step(self, lr):\n",
    "        self.weight = self.weight - self._grad_weight * lr\n",
    "        self.bias = self.bias - self._grad_bias * lr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.relu = lambda x: x * (x > 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._res = x * (x > 0)\n",
    "\n",
    "        return self._res\n",
    "\n",
    "    def backward(self, grad):\n",
    "        m, n = np.shape(self._res)\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                grad[i][j] = grad[i][j] if self.relu(self._res[i][j]) else 0\n",
    "        return grad\n",
    "\n",
    "    def step(self, lr):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(nnModule):\n",
    "    def forward(self, x):\n",
    "        self._res = 1 / (1 + np.exp(-x))\n",
    "\n",
    "        return self._res\n",
    "\n",
    "    def backward(self, grad):\n",
    "        new_grad = self._res * (1 - self._res) * grad\n",
    "\n",
    "        return new_grad\n",
    "\n",
    "    def step(self, lr):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFFN(nnModule):\n",
    "    def __init__(self) -> None:\n",
    "        self.modules = np.array([])\n",
    "\n",
    "        self.linear1 = Linear(512, 2048)\n",
    "        self.relu = ReLU()\n",
    "        self.linear2 = Linear(2048, 512)\n",
    "\n",
    "        np.append(self.modules, \n",
    "            self.linear1, \n",
    "            self.relu, \n",
    "            self.linear2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = None\n",
    "        for module in self.modules:\n",
    "            res = module.forward(x)\n",
    "        return res\n",
    "\n",
    "    def backward(self, x):\n",
    "        for module in np.flip(self.modules):\n",
    "            module.backward(x)\n",
    "\n",
    "    def step(self, lr):\n",
    "        for module in self.modules:\n",
    "            module.step(lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.99938264 0.99969132 2.99907397]\n"
     ]
    }
   ],
   "source": [
    "def ScaledDotProductAttention(Q: np.array, K: np.array, V: np.array):\n",
    "    d = K.shape[0]\n",
    "    smax = Sigmoid()\n",
    "    return smax.forward(np.dot(Q, K.T) / m.sqrt(d)) * V\n",
    "\n",
    "Q = np.array([1, 2, 3])\n",
    "K = np.array([1, 2, 3])\n",
    "V = np.array([2, 1, 3])\n",
    "\n",
    "print(ScaledDotProductAttention(Q, K, V))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.heads = params[\"heads\"]\n",
    "        self.d_model = params[\"d_model\"]\n",
    "\n",
    "        self.head_modules = np.array([])\n",
    "\n",
    "        self.linear_out = Linear(heads * V.shape[0], d_model)\n",
    "\n",
    "        for i in range(self.heads):\n",
    "            np.append(self.head_modules, [\n",
    "                Linear(d_model, Q.shape[0]), # po idee vector len = 64\n",
    "                Linear(d_model, K.shape[0]),\n",
    "                Linear(d_model, V.shape[0])\n",
    "            ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        head_res = np.array([])\n",
    "        \n",
    "        for head_module in self.head_modules:\n",
    "            Q = head_module[0].forward(x[0])\n",
    "            K = head_module[1].forward(x[1])\n",
    "            V = head_module[2].forward(x[2])\n",
    "            np.append(head_res, ScaledDotProductAttention(Q, K, V))\n",
    "\n",
    "        concat = np.vstack(*head_res)\n",
    "        out = self.linear_out(concat) # linear layer razmern' = poschitat result np.vstack\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nnModule):\n",
    "    def __init__(self, params, features, eps) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.a2 = np.ones(features)\n",
    "        self.b2 = np.zeros(features)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a2 * (x - mean) / (std + self.eps) + self.b2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "        self.self_attention = params[\"attention\"]\n",
    "        self.FFN = TransformerFFN()\n",
    "        self.layer_norm = LayerNorm() # features, eps\n",
    "        #self.dropout = Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.layer_norm(self.self_attention(x))\n",
    "        return x + self.layer_norm(self.FFN(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nnModule):\n",
    "    def __init__(self, params, layer, N) -> None:\n",
    "        super().__init__(params)\n",
    "        \n",
    "        self.layers = np.array([\n",
    "            EncoderLayer() for i in range(N)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_position(embedding):\n",
    "    return 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer():\n",
    "    def __init__(self) -> None:\n",
    "        self.multihead_attention = MultiheadAttention(params={\"heads\": heads, \"d_model\": d_model})\n",
    "        self.FFN = TransformerFFN()\n",
    "        self.linear_out = Linear() # in_dim=? out_dim=?\n",
    "        self.softmax = Sigmoid()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear: (in_dim: 384, out_dim: 24)\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(params={\"in_dim\": 384, \"out_dim\": 24})\n",
    "print(linear)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P_drop = 0.1.\n",
    "\n",
    "Label Smoothing During training, we employed label smoothing of value E_ls = 0.1. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c881db0e76e4d07227a54605e4579ac40ea89326ca18881e5cf3019b0657001f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
