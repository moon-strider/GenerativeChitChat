{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt\n",
    "import math as m\n",
    "import numpy as np\n",
    "import abc\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512               # Dimensions of the model\n",
    "heads = 8                   # Number of heads for multihead attention\n",
    "d_k = d_v = d_model / heads # ???????????????????????????????????\n",
    "ed_count = 6                # N, number of encoder-decoder layers\n",
    "vocab = [\"ji\", \"ja\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 204173.62it/s]\n"
     ]
    }
   ],
   "source": [
    "def TTV_split(data, tr, te, va, verbose=False):\n",
    "    total = len(data)\n",
    "    one = total / (tr + te + va)\n",
    "\n",
    "    tr_n = m.floor(tr * one)\n",
    "    te_n = m.floor(te * one)\n",
    "    va_n = m.floor(va * one)\n",
    "\n",
    "    tr_n += (total - tr_n - te_n - va_n)\n",
    "\n",
    "    train_data = np.array([])\n",
    "    test_data = np.array([])\n",
    "    val_data = np.array([])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    for i in tqdm(data):\n",
    "        if (len(train_data) < tr_n):\n",
    "            np.append(train_data, i)\n",
    "            continue\n",
    "        if (len(test_data) < te_n):\n",
    "            np.append(test_data, i)\n",
    "            continue\n",
    "        if (len(val_data) < va_n):\n",
    "            np.append(val_data, i)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"total = {total}\\ntr_n = {len(train_data)} \\nte_n = {len(test_data)} \\nva_n = {len(val_data)}\")\n",
    "    return \n",
    "\n",
    "TTV_split(np.array([i for i in range(100000)]), 12, 4, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Module Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnModule(): # TODO: добавить тесты для каждого модуля\n",
    "    def __init__(self, params={}) -> None:\n",
    "        self.params = params\n",
    "\n",
    "    @abc.abstractclassmethod\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abc.abstractclassmethod\n",
    "    def backward(self, grad):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abc.abstractclassmethod\n",
    "    def step(self, lr):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        rs = \"\"\n",
    "        strs = np.array([f\"{i}: {self.params[i]}, \" for i in self.params])\n",
    "        for i in strs:\n",
    "            rs += i\n",
    "        return (f\"{self.name}: (\" + rs[:len(rs)-2] + \")\") if self.params else self.name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        try:\n",
    "            if params[\"in_dim\"] < 1 or params[\"out_dim\"] < 1:\n",
    "                raise Exception\n",
    "        except KeyError:\n",
    "            raise Exception(\"You have to set in_dim and out_dim parameters for linear layer\")\n",
    "        except Exception:\n",
    "            raise Exception(\"The in_dim and out_dim have to be greater than zero\")\n",
    "\n",
    "        self.name = \"Linear\"\n",
    "\n",
    "        self._res = None\n",
    "        self._lastX = None\n",
    "\n",
    "        self.W = np.ones(params[\"in_dim\"], params[\"out_dim\"], dtype=np.float32)\n",
    "        self.B = np.zeros(params[\"out_dim\"], dtype=np.float32)\n",
    "\n",
    "        self._grad_weight = None\n",
    "        self._grad_bias = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._lastX = x\n",
    "        self._res = np.dot(x, self.W) + self.B\n",
    "        return self._res\n",
    "\n",
    "\n",
    "    def backward(self, grad):\n",
    "        self._grad_weight = np.dot(self._lastX.T, grad)\n",
    "        self._grad_bias = np.sum(grad, axis=0)\n",
    "\n",
    "\n",
    "    def step(self, lr):\n",
    "        self.weight = self.weight - self._grad_weight * lr\n",
    "        self.bias = self.bias - self._grad_bias * lr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"ReLU\"\n",
    "\n",
    "        self.relu = lambda x: x * (x > 0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._res = x * (x > 0)\n",
    "        return self._res\n",
    "\n",
    "\n",
    "    def backward(self, grad):\n",
    "        m, n = np.shape(self._res)\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                grad[i][j] = grad[i][j] if self.relu(self._res[i][j]) else 0\n",
    "        return grad\n",
    "\n",
    "\n",
    "    def step(self, lr):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(nnModule): # dobavit dim over -1?\n",
    "    def __init__(self, params=...) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Sigmoid\"\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._res = 1 / (1 + np.exp(-x))\n",
    "        return self._res\n",
    "\n",
    "\n",
    "    def backward(self, grad):\n",
    "        new_grad = self._res * (1 - self._res) * grad\n",
    "        return new_grad\n",
    "\n",
    "\n",
    "    def step(self, lr):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFFN(nnModule):\n",
    "    def __init__(self) -> None:\n",
    "        self.modules = np.array([])\n",
    "\n",
    "        self.name = \"FFN\"\n",
    "\n",
    "        self.linear1 = Linear(512, 2048)\n",
    "        self.relu = ReLU()\n",
    "        self.linear2 = Linear(2048, 512)\n",
    "\n",
    "        np.append(self.modules, \n",
    "            self.linear1, \n",
    "            self.relu, \n",
    "            self.linear2\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = None\n",
    "        for module in self.modules:\n",
    "            res = module.forward(x)\n",
    "        return res\n",
    "\n",
    "\n",
    "    def backward(self, x):\n",
    "        for module in np.flip(self.modules):\n",
    "            module.backward(x)\n",
    "\n",
    "\n",
    "    def step(self, lr):\n",
    "        for module in self.modules:\n",
    "            module.step(lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.99938264 0.99969132 2.99907397]\n"
     ]
    }
   ],
   "source": [
    "def ScaledDotProductAttention(Q: np.array, K: np.array, V: np.array):\n",
    "    d = K.shape[0]\n",
    "    smax = Sigmoid()\n",
    "    return smax.forward(np.dot(Q, K.T) / m.sqrt(d)) * V\n",
    "\n",
    "Q = np.array([1, 2, 3])\n",
    "K = np.array([1, 2, 3])\n",
    "V = np.array([2, 1, 3])\n",
    "\n",
    "print(ScaledDotProductAttention(Q, K, V))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Multihead Attention\"\n",
    "\n",
    "        self.heads = params[\"heads\"]\n",
    "        self.d_model = params[\"d_model\"]\n",
    "\n",
    "        assert not (self.d_model % self.heads)\n",
    "\n",
    "        self.head_modules = np.array([\n",
    "            [\n",
    "                Linear(params={\n",
    "                    \"in_dim\": d_model, \n",
    "                    \"out_dim\": Q.shape[0]\n",
    "                }), # po idee vector len = 64\n",
    "                Linear(params={\n",
    "                    \"in_dim\": d_model, \n",
    "                    \"out_dim\": K.shape[0]\n",
    "                }),\n",
    "                Linear(params={\n",
    "                    \"in_dim\": d_model, \n",
    "                    \"out_dim\": V.shape[0]\n",
    "                }),\n",
    "            ] for i in range(self.heads)\n",
    "        ])\n",
    "\n",
    "        self.linear_out = Linear(heads * V.shape[0], d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        head_res = np.array([])\n",
    "        \n",
    "        for head_module in self.head_modules:\n",
    "            Q = head_module[0].forward(x[0])\n",
    "            K = head_module[1].forward(x[1])\n",
    "            V = head_module[2].forward(x[2])\n",
    "            np.append(head_res, ScaledDotProductAttention(Q, K, V))\n",
    "\n",
    "        concat = np.vstack(*head_res)\n",
    "        out = self.linear_out(concat) # linear layer razmern' = poschitat result np.vstack\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Layer Norm\"\n",
    "\n",
    "        self.features = params[\"features\"]\n",
    "        self.eps = params[\"eps\"]\n",
    "        \n",
    "        self.a_2 = np.ones(self.features)\n",
    "        self.b_2 = np.zeros(self.features)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Encoder Layer\"\n",
    "\n",
    "        self.self_attention = params[\"attention\"]\n",
    "        self.FFN = TransformerFFN()\n",
    "        self.layer_norm = LayerNorm() # features, eps\n",
    "        # self.dropout = Dropout()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.layer_norm(self.self_attention(x))\n",
    "        return x + self.layer_norm(self.FFN(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nnModule):\n",
    "    def __init__(self, params, N) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Encoder\"\n",
    "\n",
    "        self.attention = params[\"attention\"]\n",
    "\n",
    "        self.layers = np.array([\n",
    "            EncoderLayer(params={\n",
    "                \"attention\": self.attention\n",
    "            }) for i in range(N)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nnModule):   # TODO: remove inconsistensies of parameters vs creating objects inside\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Decoder Layer\"\n",
    "\n",
    "        self.memory = params[\"memory\"] # ????????\n",
    "        self.heads = params[\"heads\"]\n",
    "        self.d_model = params[\"d_model\"]\n",
    "\n",
    "        self.self_attention_1 = MultiheadAttention(params={\n",
    "            \"heads\": self.heads, \n",
    "            \"d_model\": self.d_model\n",
    "        })\n",
    "        self.self_attention_2 = MultiheadAttention(params={\n",
    "            \"heads\": self.heads, \n",
    "            \"d_model\": self.d_model\n",
    "        })\n",
    "\n",
    "        self.layer_norm = LayerNorm() # layer size\n",
    "\n",
    "        self.FFN = TransformerFFN()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.layer_norm(self.self_attention_1(x))\n",
    "        x = x + self.layer_norm(self.memory + self.self_attention_2(x))\n",
    "        return x + self.layer_norm(self.FFN(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nnModule):\n",
    "    def __init__(self, params, N) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Decoder\"\n",
    "\n",
    "        self.layers = np.array([\n",
    "            DecoderLayer(params={\n",
    "                \"memory\": object,\n",
    "                \"heads\": heads,\n",
    "                \"d_model\": d_model\n",
    "            }) for i in range(N)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EncoderDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Encoder-Decoder\"\n",
    "\n",
    "        self.encoder = params[\"encoder\"]\n",
    "        self.decoder = params[\"decoder\"]\n",
    "        self.src_emb = params[\"src_emb\"]\n",
    "        self.tar_emb = params[\"tar_emb\"]\n",
    "        \n",
    "        self.generator = params[\"generator\"]\n",
    "\n",
    "\n",
    "    def encode(self, src):\n",
    "        return self.encoder(self.src_emb(src))\n",
    "\n",
    "\n",
    "    def decode(self, src, memory):\n",
    "        return self.decoder(self.tar_emb(self.tar), memory)\n",
    "\n",
    "    \n",
    "    def forward(self, src, tar):\n",
    "        return self.decode(self.encode(src), tar)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.d_model = params[\"d_model\"]\n",
    "        self.vocab = params[\"vocab\"]\n",
    "\n",
    "        self.smax = Sigmoid()\n",
    "        self.linear = Linear(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.smax(self.linear(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Transformer\"\n",
    "\n",
    "        self.encoder_decoder = params[\"encoder-decoder\"]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder_decoder(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ones() got multiple values for argument 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m encoder \u001b[39m=\u001b[39m Encoder(params\u001b[39m=\u001b[39m{\n\u001b[0;32m----> 2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mattention\u001b[39m\u001b[39m\"\u001b[39m: MultiheadAttention(params\u001b[39m=\u001b[39;49m{\n\u001b[1;32m      3\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mheads\u001b[39;49m\u001b[39m\"\u001b[39;49m: heads, \n\u001b[1;32m      4\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39md_model\u001b[39;49m\u001b[39m\"\u001b[39;49m: d_model\n\u001b[1;32m      5\u001b[0m     })\n\u001b[1;32m      6\u001b[0m })\n\u001b[1;32m      7\u001b[0m decoder \u001b[39m=\u001b[39m Decoder()\n\u001b[1;32m      9\u001b[0m generator \u001b[39m=\u001b[39m Generator(params\u001b[39m=\u001b[39m{\n\u001b[1;32m     10\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39md_model\u001b[39m\u001b[39m\"\u001b[39m: d_model,\n\u001b[1;32m     11\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m: vocab\n\u001b[1;32m     12\u001b[0m })\n",
      "Cell \u001b[0;32mIn [55], line 12\u001b[0m, in \u001b[0;36mMultiheadAttention.__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39md_model\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads)\n\u001b[0;32m---> 12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_modules \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\n\u001b[1;32m     13\u001b[0m     [\n\u001b[1;32m     14\u001b[0m         Linear(params\u001b[39m=\u001b[39m{\n\u001b[1;32m     15\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39min_dim\u001b[39m\u001b[39m\"\u001b[39m: d_model, \n\u001b[1;32m     16\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mout_dim\u001b[39m\u001b[39m\"\u001b[39m: Q\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     17\u001b[0m         }), \u001b[39m# po idee vector len = 64\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         Linear(params\u001b[39m=\u001b[39m{\n\u001b[1;32m     19\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39min_dim\u001b[39m\u001b[39m\"\u001b[39m: d_model, \n\u001b[1;32m     20\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mout_dim\u001b[39m\u001b[39m\"\u001b[39m: K\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m         }),\n\u001b[1;32m     22\u001b[0m         Linear(params\u001b[39m=\u001b[39m{\n\u001b[1;32m     23\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39min_dim\u001b[39m\u001b[39m\"\u001b[39m: d_model, \n\u001b[1;32m     24\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mout_dim\u001b[39m\u001b[39m\"\u001b[39m: V\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     25\u001b[0m         }),\n\u001b[1;32m     26\u001b[0m     ] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads)\n\u001b[1;32m     27\u001b[0m ])\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_out \u001b[39m=\u001b[39m Linear(heads \u001b[39m*\u001b[39m V\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], d_model)\n",
      "Cell \u001b[0;32mIn [55], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39md_model\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads)\n\u001b[1;32m     12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_modules \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\n\u001b[1;32m     13\u001b[0m     [\n\u001b[0;32m---> 14\u001b[0m         Linear(params\u001b[39m=\u001b[39;49m{\n\u001b[1;32m     15\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39min_dim\u001b[39;49m\u001b[39m\"\u001b[39;49m: d_model, \n\u001b[1;32m     16\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mout_dim\u001b[39;49m\u001b[39m\"\u001b[39;49m: Q\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m     17\u001b[0m         }), \u001b[39m# po idee vector len = 64\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         Linear(params\u001b[39m=\u001b[39m{\n\u001b[1;32m     19\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39min_dim\u001b[39m\u001b[39m\"\u001b[39m: d_model, \n\u001b[1;32m     20\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mout_dim\u001b[39m\u001b[39m\"\u001b[39m: K\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m         }),\n\u001b[1;32m     22\u001b[0m         Linear(params\u001b[39m=\u001b[39m{\n\u001b[1;32m     23\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39min_dim\u001b[39m\u001b[39m\"\u001b[39m: d_model, \n\u001b[1;32m     24\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mout_dim\u001b[39m\u001b[39m\"\u001b[39m: V\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     25\u001b[0m         }),\n\u001b[1;32m     26\u001b[0m     ] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads)\n\u001b[1;32m     27\u001b[0m ])\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_out \u001b[39m=\u001b[39m Linear(heads \u001b[39m*\u001b[39m V\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], d_model)\n",
      "Cell \u001b[0;32mIn [37], line 18\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_res \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lastX \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mones(params[\u001b[39m\"\u001b[39;49m\u001b[39min_dim\u001b[39;49m\u001b[39m\"\u001b[39;49m], params[\u001b[39m\"\u001b[39;49m\u001b[39mout_dim\u001b[39;49m\u001b[39m\"\u001b[39;49m], dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat32)\n\u001b[1;32m     19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mB \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(params[\u001b[39m\"\u001b[39m\u001b[39mout_dim\u001b[39m\u001b[39m\"\u001b[39m], dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_grad_weight \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: ones() got multiple values for argument 'dtype'"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(params={\n",
    "    \"attention\": MultiheadAttention(params={\n",
    "        \"heads\": heads, \n",
    "        \"d_model\": d_model\n",
    "    })\n",
    "})\n",
    "decoder = Decoder()\n",
    "\n",
    "generator = Generator(params={\n",
    "    \"d_model\": d_model,\n",
    "    \"vocab\": vocab\n",
    "})\n",
    "\n",
    "endec = EncoderDecoder(params={\n",
    "    \"encoder\": encoder,\n",
    "    \"decoder\": decoder,\n",
    "    \"src_emb\": object,      # ?????\n",
    "    \"tar_emb\": object       # ?????\n",
    "})\n",
    "\n",
    "model = Transformer(params={\"encoder-decoder\": endec})\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P_drop = 0.1.\n",
    "\n",
    "Label Smoothing During training, we employed label smoothing of value E_ls = 0.1. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c881db0e76e4d07227a54605e4579ac40ea89326ca18881e5cf3019b0657001f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
