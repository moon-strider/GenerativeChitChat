{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt\n",
    "import math as m\n",
    "import numpy as np\n",
    "import abc\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512               # Dimensions of the model\n",
    "heads = 8                   # Number of heads for multihead attention\n",
    "d_k = d_v = d_model / heads # ???????????????????????????????????\n",
    "ed_count = 6                # N, number of encoder-decoder layers\n",
    "vocab = [\"ji\", \"ja\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 181236.20it/s]\n"
     ]
    }
   ],
   "source": [
    "def TTV_split(data, tr, te, va, verbose=False):\n",
    "    total = len(data)\n",
    "    one = total / (tr + te + va)\n",
    "\n",
    "    tr_n = m.floor(tr * one)\n",
    "    te_n = m.floor(te * one)\n",
    "    va_n = m.floor(va * one)\n",
    "\n",
    "    tr_n += (total - tr_n - te_n - va_n)\n",
    "\n",
    "    train_data = np.array([])\n",
    "    test_data = np.array([])\n",
    "    val_data = np.array([])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    for i in tqdm(data):\n",
    "        if (len(train_data) < tr_n):\n",
    "            np.append(train_data, i)\n",
    "            continue\n",
    "        if (len(test_data) < te_n):\n",
    "            np.append(test_data, i)\n",
    "            continue\n",
    "        if (len(val_data) < va_n):\n",
    "            np.append(val_data, i)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"total = {total}\\ntr_n = {len(train_data)} \\nte_n = {len(test_data)} \\nva_n = {len(val_data)}\")\n",
    "    return \n",
    "\n",
    "TTV_split(np.array([i for i in range(100000)]), 12, 4, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Module Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnModule(): # TODO: добавить тесты для каждого модуля\n",
    "    def __init__(self, params={}) -> None:\n",
    "        self.params = params\n",
    "\n",
    "    @abc.abstractclassmethod\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abc.abstractclassmethod\n",
    "    def backward(self, grad):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abc.abstractclassmethod\n",
    "    def step(self, lr):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        rs = \"\"\n",
    "        strs = np.array([f\"\\n{i}: {self.params[i]}, \" for i in self.params])\n",
    "        for i in strs:\n",
    "            rs += i\n",
    "        return (f\"{self.name}: (\" + rs[:len(rs)-2] + \"\\n)\") if self.params else self.name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        try:\n",
    "            if params[\"in_dim\"] < 1 or params[\"out_dim\"] < 1:\n",
    "                raise Exception\n",
    "        except KeyError:\n",
    "            raise Exception(\"You have to set in_dim and out_dim parameters for linear layer\")\n",
    "        except Exception:\n",
    "            raise Exception(\"The in_dim and out_dim have to be greater than zero\")\n",
    "\n",
    "        self.name = \"Linear\"\n",
    "\n",
    "        self._res = None\n",
    "        self._lastX = None\n",
    "\n",
    "        self.W = np.ones((params[\"in_dim\"], params[\"out_dim\"]), dtype=np.float64)\n",
    "        self.B = np.zeros(params[\"out_dim\"], dtype=np.float64)\n",
    "\n",
    "        self._grad_weight = None\n",
    "        self._grad_bias = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._lastX = x\n",
    "        self._res = np.dot(x, self.W) + self.B\n",
    "        return self._res\n",
    "\n",
    "\n",
    "    def backward(self, grad):\n",
    "        self._grad_weight = np.dot(self._lastX.T, grad)\n",
    "        self._grad_bias = np.sum(grad, axis=0)\n",
    "\n",
    "\n",
    "    def step(self, lr):\n",
    "        self.weight = self.weight - self._grad_weight * lr\n",
    "        self.bias = self.bias - self._grad_bias * lr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(nnModule):\n",
    "    def __init__(self, params=...) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"ReLU\"\n",
    "\n",
    "        self.relu = lambda x: x * (x > 0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._res = x * (x > 0)\n",
    "        return self._res\n",
    "\n",
    "\n",
    "    def backward(self, grad):\n",
    "        m, n = np.shape(self._res)\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                grad[i][j] = grad[i][j] if self.relu(self._res[i][j]) else 0\n",
    "        return grad\n",
    "\n",
    "\n",
    "    def step(self, lr):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(nnModule): # dobavit dim over -1?\n",
    "    def __init__(self, params=...) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Sigmoid\"\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._res = 1 / (1 + np.exp(-x))\n",
    "        return self._res\n",
    "\n",
    "\n",
    "    def backward(self, grad):\n",
    "        new_grad = self._res * (1 - self._res) * grad\n",
    "        return new_grad\n",
    "\n",
    "\n",
    "    def step(self, lr):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFFN(nnModule):\n",
    "    def __init__(self) -> None:\n",
    "        self.name = \"FFN\"\n",
    "\n",
    "        self.linear1 = Linear(params={\n",
    "            \"in_dim\": 512, \n",
    "            \"out_dim\": 2048\n",
    "        })\n",
    "\n",
    "        self.relu = ReLU()\n",
    "        \n",
    "        self.linear2 = Linear(params={\n",
    "            \"in_dim\": 2048, \n",
    "            \"out_dim\": 512\n",
    "        })\n",
    "\n",
    "        self.modules = np.array([\n",
    "            self.linear1, \n",
    "            self.relu, \n",
    "            self.linear2\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = None\n",
    "        for module in self.modules:\n",
    "            res = module.forward(x)\n",
    "        return res\n",
    "\n",
    "\n",
    "    def backward(self, x):\n",
    "        for module in np.flip(self.modules):\n",
    "            module.backward(x)\n",
    "\n",
    "\n",
    "    def step(self, lr):\n",
    "        for module in self.modules:\n",
    "            module.step(lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.99938264 0.99969132 2.99907397]\n"
     ]
    }
   ],
   "source": [
    "def ScaledDotProductAttention(Q: np.array, K: np.array, V: np.array):\n",
    "    d = K.shape[0]\n",
    "    smax = Sigmoid()\n",
    "    return smax.forward(np.dot(Q, K.T) / m.sqrt(d)) * V\n",
    "\n",
    "Q = np.array([1, 2, 3])\n",
    "K = np.array([1, 2, 3])\n",
    "V = np.array([2, 1, 3])\n",
    "\n",
    "print(ScaledDotProductAttention(Q, K, V))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Multihead Attention\"\n",
    "\n",
    "        self.heads = params[\"heads\"]\n",
    "        self.d_model = params[\"d_model\"]\n",
    "\n",
    "        assert not (self.d_model % self.heads)\n",
    "\n",
    "        self.head_modules = np.array([\n",
    "            [\n",
    "                Linear(params={\n",
    "                    \"in_dim\": d_model, \n",
    "                    \"out_dim\": Q.shape[0]\n",
    "                }), # po idee vector len = 64\n",
    "                Linear(params={\n",
    "                    \"in_dim\": d_model, \n",
    "                    \"out_dim\": K.shape[0]\n",
    "                }),\n",
    "                Linear(params={\n",
    "                    \"in_dim\": d_model, \n",
    "                    \"out_dim\": V.shape[0]\n",
    "                }),\n",
    "            ] for i in range(self.heads)\n",
    "        ])\n",
    "\n",
    "        self.linear_out = Linear(params={\n",
    "            \"in_dim\": heads * V.shape[0], \n",
    "            \"out_dim\": d_model\n",
    "        })\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        head_res = np.array([])\n",
    "        \n",
    "        for head_module in self.head_modules:\n",
    "            Q = head_module[0].forward(x[0])\n",
    "            K = head_module[1].forward(x[1])\n",
    "            V = head_module[2].forward(x[2])\n",
    "            np.append(head_res, ScaledDotProductAttention(Q, K, V))\n",
    "\n",
    "        concat = np.vstack(*head_res)\n",
    "        out = self.linear_out(concat) # linear layer razmern' = poschitat result np.vstack\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Layer Norm\"\n",
    "\n",
    "        self.features = params[\"features\"]\n",
    "        self.eps = params[\"eps\"]\n",
    "        \n",
    "        self.a_2 = np.ones(self.features)\n",
    "        self.b_2 = np.zeros(self.features)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Encoder Layer\"\n",
    "\n",
    "        self.self_attention = params[\"attention\"]\n",
    "        self.FFN = TransformerFFN()\n",
    "        self.layer_norm = LayerNorm(params={\n",
    "            \"features\": 512, # TODO: fix\n",
    "            \"eps\": 0.0001\n",
    "        })\n",
    "        # self.dropout = Dropout()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.layer_norm(self.self_attention(x))\n",
    "        return x + self.layer_norm(self.FFN(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Encoder\"\n",
    "\n",
    "        self.attention = params[\"attention\"]\n",
    "        self.N = ed_count\n",
    "\n",
    "        self.layers = np.array([\n",
    "            EncoderLayer(params={\n",
    "                \"attention\": self.attention\n",
    "            }) for i in range(self.N)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nnModule):   # TODO: remove inconsistensies of parameters vs creating objects inside\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Decoder Layer\"\n",
    "\n",
    "        self.memory = params[\"memory\"] # ????????\n",
    "        self.heads = params[\"heads\"]\n",
    "        self.d_model = params[\"d_model\"]\n",
    "\n",
    "        self.self_attention_1 = MultiheadAttention(params={\n",
    "            \"heads\": self.heads, \n",
    "            \"d_model\": self.d_model\n",
    "        })\n",
    "        self.self_attention_2 = MultiheadAttention(params={\n",
    "            \"heads\": self.heads, \n",
    "            \"d_model\": self.d_model\n",
    "        })\n",
    "\n",
    "        self.layer_norm = LayerNorm(params={\n",
    "            \"features\": 512, # TODO: fix\n",
    "            \"eps\": 0.0001\n",
    "        }) # layer size\n",
    "\n",
    "        self.FFN = TransformerFFN()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.layer_norm(self.self_attention_1(x))\n",
    "        x = x + self.layer_norm(self.memory + self.self_attention_2(x))\n",
    "        return x + self.layer_norm(self.FFN(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Decoder\"\n",
    "\n",
    "        self.N = params[\"N\"]\n",
    "\n",
    "        self.layers = np.array([\n",
    "            DecoderLayer(params={\n",
    "                \"memory\": object,\n",
    "                \"heads\": heads,\n",
    "                \"d_model\": d_model\n",
    "            }) for i in range(self.N)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EncoderDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Encoder-Decoder\"\n",
    "\n",
    "        self.encoder = params[\"encoder\"]\n",
    "        self.decoder = params[\"decoder\"]\n",
    "        self.src_emb = params[\"src_emb\"]\n",
    "        self.tar_emb = params[\"tar_emb\"]\n",
    "        \n",
    "        self.generator = params[\"generator\"]\n",
    "\n",
    "\n",
    "    def encode(self, src):\n",
    "        return self.encoder(self.src_emb(src))\n",
    "\n",
    "\n",
    "    def decode(self, src, memory):\n",
    "        return self.decoder(self.tar_emb(self.tar), memory)\n",
    "\n",
    "    \n",
    "    def forward(self, src, tar):\n",
    "        return self.decode(self.encode(src), tar)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Output generator\"\n",
    "\n",
    "        self.d_model = params[\"d_model\"]\n",
    "        self.vocab = params[\"vocab\"]\n",
    "\n",
    "        self.smax = Sigmoid()\n",
    "        self.linear = Linear(params={ # d_model in_dim????\n",
    "            \"in_dim\": d_model, \n",
    "            \"out_dim\": d_model\n",
    "        })\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.smax(self.linear(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nnModule):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.name = \"Transformer\"\n",
    "\n",
    "        self.encoder_decoder = params[\"encoder-decoder\"]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder_decoder(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer: (\n",
      "encoder-decoder: Encoder-Decoder: (\n",
      "generator: Output generator: (\n",
      "d_model: 512, \n",
      "vocab: ['ji', 'ja']\n",
      "), \n",
      "encoder: Encoder: (\n",
      "attention: Multihead Attention: (\n",
      "heads: 8, \n",
      "d_model: 512\n",
      ")\n",
      "), \n",
      "decoder: Decoder: (\n",
      "N: 6\n",
      "), \n",
      "src_emb: <class 'object'>, \n",
      "tar_emb: <class 'object'>\n",
      ")\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(params={\n",
    "    \"attention\": MultiheadAttention(params={\n",
    "        \"heads\": heads, \n",
    "        \"d_model\": d_model\n",
    "    })\n",
    "})\n",
    "decoder = Decoder(params={\n",
    "    \"N\": ed_count\n",
    "})\n",
    "\n",
    "generator = Generator(params={\n",
    "    \"d_model\": d_model,\n",
    "    \"vocab\": vocab\n",
    "})\n",
    "\n",
    "endec = EncoderDecoder(params={\n",
    "    \"generator\": generator,\n",
    "    \"encoder\": encoder,\n",
    "    \"decoder\": decoder,\n",
    "    \"src_emb\": object,      # ?????\n",
    "    \"tar_emb\": object       # ?????\n",
    "})\n",
    "\n",
    "model = Transformer(params={\"encoder-decoder\": endec})\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P_drop = 0.1.\n",
    "\n",
    "Label Smoothing During training, we employed label smoothing of value E_ls = 0.1. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c881db0e76e4d07227a54605e4579ac40ea89326ca18881e5cf3019b0657001f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
